---
title: "SVM classification of heart disease dataset "
author: "Soundarya G"
date: "25/04/2021"
output: 
  html_document:
      toc: true # table of content true
      toc_depth: 3  # upto three depths of headings (specified by #, ## and ###)
      number_sections: false  ## if you want number sections at each table header
      theme: united  # many options for theme, this one is my favorite.
      highlight: tango  # specifies the syntax highlighting style
---

# INTRODUCTION

**Aim of analysis**
    
    In the following document, I will be using SVM classification techinque to predict heart disease (angiographic disease status). From a set of 14 variables, the most important to predict heart failure are whether or not there is a reversable defect in Thalassemia followed by whether or not there is an occurrence of asymptomatic chest pain.

## 1. Load the necessary packages 

```{r}
require(ggplot2)
require(pROC) #to plot the ROC curves
require(caret)

library(tidyverse)    # data manipulation and visualization
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(ISLR)         # contains example data set "Khan"
library(RColorBrewer) # customized coloring of plots
```

## 2. Load the dataset

    The heart disease data are available at UCI and Kaggle.

```{r}
heartdf <- read.csv("heart.csv")
```

# EXPLORATORY DATA ANALYSIS

### 1.Columns and shape of dataset 

```{r}
names(heartdf) <- c( "age", "sex", "cp", "trestbps", "chol","fbs", "restecg", "thalach","exang", "oldpeak","slope", "ca", "thal", "num")
attach(heartdf)
```


```{r}
# dimensions of the dataset
dim(heartdf)
```

    The variable we want to predict is num with Value 0: < 50% diameter narrowing and Value 1: > 50% diameter narrowing. We assume that every value with 0 means heart is okay, and 1,2,3,4 means heart disease.

    From the possible values the variables can take, it is evident that the following need to be dummified because the distances in the values is random: cp,thal, restecg, slope


### 2. Viewing dataset

```{r}
head(heartdf,5)
```

    Explore the data and find how many had heart attacks, women or men have of a particular age?

```{r}
#converting the num variable to binary class variable

heartdf$num<-ifelse(heartdf$num > 0,"Disease","noDisease")
table(heartdf$num)
```


```{r}
#distribution of the target variable
ggplot(heartdf,aes(x = num)) + geom_bar(fill="dark green")
```

### 3. Conversion factor variable

```{r}
#converting to factor variable

heartdf$sex<-ifelse(heartdf$sex==0,"female","male")

table(heartdf$sex)
```


```{r}
table(sex=heartdf$sex,disease=heartdf$num)
```

```{r}
ggplot(heartdf,aes(x=sex)) + geom_bar(fill="brown") + facet_wrap(~num)
```

### 4. Box plot for statistical distribution


```{r}
#heart disease and age

by(heartdf$age,heartdf$num,summary)
```

    -So people who had heart disease for them the mean age is 52.5

```{r}
ggplot(heartdf,aes(x = num,y = age)) + geom_boxplot()
```

### 5. Correlation analysis between some variables

```{r}
#very low correlation
cor.test(age,chol) 
```

### 6. Confusion matrix


```{r}
cor.test(age,thalach)
```

```{r}
ggplot(heartdf,aes(x = age,y = thalach )) + geom_point() + geom_smooth()
```

    -Here we can notice as age increase maximum heart rate achived descreases, as the cor-relation is negetive.

# Model Building

### 1. Data splitting

```{r}
set.seed(5)

inTrainRows <- createDataPartition(heartdf$num,p=0.8,list=FALSE)

trainData <- heartdf[inTrainRows,]
testData <- heartdf[-inTrainRows,]

nrow(trainData)/(nrow(testData)+nrow(trainData)) 
```

### 2. Feature selections

```{r}
# for this to work add names to all levels (numbers not allowed)
feature.names=names(heartdf)
for (f in feature.names) {
  if (class(heartdf[[f]])=="factor") {
      levels <- unique(c(heartdf[[f]]))
      heartdf[[f]] <- factor(heartdf[[f]],
      labels=make.names(levels))
  }
}
```


```{r}
#converting to factor variable with 2 levels

heartdf$num<-as.factor(heartdf$num)
levels(heartdf$num) <- c("Notdisease","Disease")

table(heartdf$num)
```

### 3. Building a SVM classifier
  
    Now SVM classifier tends to generate hyperplanes which separate the classes with maximum margins i.e in simpler terms it aims to generate maximum marginal hyperplane.

```{r}
set.seed(10)
inTrainRows <- createDataPartition(heartdf$num,p=0.7,list=FALSE)
trainData2 <- heartdf[inTrainRows,]
testData2 <- heartdf[-inTrainRows,]
```

#### 1. Method Radial
```{r}
#cross validation
fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 10,
                          ## Estimate class probabilities
                          classProbs = TRUE,
                          ## Evaluate performance using
                          ## the following function
                          summaryFunction = twoClassSummary)
```


```{r}
svm_radial <- train(num ~ ., data = na.omit(trainData2), 
                  method = "svmRadial",
                  trControl = fitControl,
                  preProcess = c("center", "scale"),
                  tuneLength = 8,
                  metric = "ROC")

svm_radial
```


```{r}
#prediction on test data-class labels
svmPrediction <- predict(svm_radial, testData2)


#probability of no heart disease-finding probabilities value
svmPredictionprob <- predict(svm_radial, testData2, type='prob')[2]


#generating a confusion matrix
ConfMatrixPrediction <- confusionMatrix(svmPrediction, na.omit(testData2)$num)
ConfMatrixPrediction$table
```

    -In the confusion matrix the diagonals represent the correctly classified examples, whereas the offdiagonals are incorrectly classifier examples.
    -To find the ROC curver and the AUC value to better understand the accuracy and performance
    -ROC curve is the plot of True positive rate vs the false positive rate.


```{r}
#ROC and AUC value

AUC<- roc(na.omit(testData2)$num,as.numeric(as.matrix((svmPredictionprob))))$auc

Accuracy<- ConfMatrixPrediction$overall['Accuracy']

svmPerformance<-cbind(AUC,Accuracy)
svmPerformance
```

    Hence we get an AUC value of 0.9258337 and overall prediction accuracy of 0.8555556.


```{r}
auc_roc<-roc(na.omit(testData2)$num,as.numeric(as.matrix((svmPredictionprob))))
plot(auc_roc)
```


#### 2. Method Linear

```{r}
#before train your model implement  trainControl method
trctrl <- trainControl(method = "repeatedcv", number=10, repeats = 3) #number is number of iteration, repeat the cross validation

svm_linear <- train(num~. , data=na.omit(trainData2), method = "svmLinear",
                    trControl = trctrl,
                    preProcess = c("center", "scale"),  
                    tuneLength=10)

#preprocessing parameter help in scaling and centering data
svm_linear
```

```{r}
test_pred <- predict(svm_linear, newdata= testData2)   # first paramter is our train model
summary(test_pred)
```

```{r}
confusionMatrix(test_pred,as.factor(testData2$num))
```

```{r}
grid <- expand.grid(C = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2,5))
svmgrid <- train(num ~., data = trainData2, method = "svmLinear",
                         trControl=trctrl,
                         preProcess = c("center", "scale"),
                         tuneGrid = grid,
                         tuneLength = 10)
svmgrid
```

```{r}
plot(svmgrid) 
```

```{r}
test_pred <- predict(svmgrid, newdata= testData2)
summary(test_pred)  
```

```{r}
confusionMatrix(test_pred,as.factor(testData2$num))
```

